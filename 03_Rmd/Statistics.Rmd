# Statistical Exploration

## Libraries
```{r message=FALSE, warning=FALSE}
library(lme4)
library(ggplot2)
library(hrbrthemes)
library(reshape2)
library(dplyr)
library(tidyr)
library(readr)
library(here)
```

```{r load_data, message=FALSE}
RT_data <- read_csv(here("05_output", "reading_times.csv"), col_names=TRUE, id=NULL)
```

```{r participant_count}
aggregate(participant_id ~ lang, data = RT_data, FUN = function(x) length(unique(x)))
```
```{r total_participants}
length(unique(RT_data$participant_id))
```

## Declare variables
```{r select_textnr}
#article <- RT_data %>% filter(textnr == 1)
article <- RT_data
```

"As regards very long fixations, deletion (but not merger) is also common. Eye-tracking researchers generally remove fixations longer than 800 ms, as recommended by Conklin and Pellicer-SÃ¡nchez (2016) and Keating (2014), because these fixations are believed to signal a lapse in attention." (Godfroid 2020: 261)
```{r outlier}
# Cap the values in the dur column to a maximum of 800
#article$dur <- pmin(article$dur, 800)

# Remove duration over 800
#article <- article[article$dur <= 800, ]
```


```{r dependent_var}
article <- article %>% mutate(log_dur = log(dur)) # log transform

article$dur[is.na(article$dur)] <- 0 # set NA to 0 
article$log_dur[is.na(article$log_dur)] <- 0

duration = article$dur
log_duration = article$log_dur
```

There are also AAPz scores for stopwords. The following code snippet changes all AAPz scores for stopwords to 0.
```{r independent_var}
# Update the AAPz column where is_stopword is 1
article$AAPz[article$is_stopword == 1] <- 0

valency = article$AAPz
```

```{r confounding_var}
# lexical-level 
word_length = article$word_length
word_frequency = article$frequency
#predictability = TBD
POS = article$pos

# participant-level
participant = article$participant_id
```

How do I get bigrams?
```{r surprisal, eval=FALSE, include=FALSE}
bnc_size = 97626093

article <- article %>% 
  mutate(
    word = frequency,
    previous_word = lag(frequency),
    conditional_prob = 0,
    bigram_surprisal = log(1 / (word / bnc_size)) + log(1 / conditional_prob)
  )
```

```{r add_column}
article <- article %>% mutate(L1 = ifelse(lang == "en", "L1", "L2"))
```

```{r change_valency}
# Turn into categorical
positive = 0.1
negative = -0.1

article <- article %>%
  mutate(AAPz_cat = case_when(
    AAPz > positive ~ "Positive",
    AAPz < negative ~ "Negative",
    TRUE ~ "Neutral"))
```

```{r organize_columns}
article <- article %>% select(textnr, lang, L1, participant_id, 
                              tokenindex, token,
                              skip, pos, is_stopword,
                              dur, log_dur,
                              AAPz, AAPz_cat, anew_score,
                              punctuation, word_length, frequency, 
                              motiv, lextale, cft20, 
                              start_learning,
                              exposure_EN
                              )

article$exposure_EN[article$lang == "en"] <- 100

# fluent, reading, reading_fluent, exposure_L1,
# country, family, `school/workplace`,
# spelling, TOWRE_word, TOWRE_nonword, vocab, vocab.t2.5
```

### Log-Transformation
Note: Values that are *NA* must be dealt with. There are multiple options for handling missing values. 
While they could be removed, this would lead to a loss of data. Another approach is to set it to a very low value, for example 1ms.
```{r}
hist(duration)
hist(log_duration)
```

### Means and Medians 
#### Across all readers for each word
```{r calc_all}
article %>%
  group_by(token, tokenindex) %>%
  summarise(meanDurAll = mean(dur), medianDurAll = median(dur))

df1 <- article %>%
    group_by(token, tokenindex) %>%
    mutate(meanDurAll = mean(dur), medianDurAll = median(dur)) %>%
    ungroup()  # Ungroup to revert to default behavior

# View the resulting data frame
head(df1)
```

Median duration of each token for each language group-
```{r calc_language}
median_duration <- article %>%
  group_by(tokenindex, lang, token) %>%
  summarise(medianDur = median(dur)) %>%
  spread(lang, medianDur, sep = "_") %>%
  rename_with(~gsub("^lang_", "median_", .), starts_with("lang_")) %>%
  rename(tokenindex = tokenindex, token = token)
```

```{r join_results}
article <- left_join(article, median_duration, by = c("token", "tokenindex"))
rm(median_duration)
```

## Mean Duration 
```{r mean_lang}
article %>% group_by(lang) %>% summarize(mean_duration = mean(dur), standard_deviation = sd(dur))
```

```{r}
# Basic boxplot
#boxplot(article$dur ~ article$lang , col=terrain.colors(9))

ggplot(article, aes(x = lang, y = dur, fill = lang)) +
  geom_boxplot() +
  scale_y_continuous(limits = c(0, 1000)) +
  scale_fill_manual(values = terrain.colors(9)) +
  labs(title = "Boxplot of Duration by Language",
       x = "Language",
       y = "Duration")

```

```{r mean_participant}
df2 <- article %>% group_by(participant_id) %>% summarize(mean_duration = mean(dur), L1)

df2[df2$mean_duration == max(df2$mean_duration), ] # slowest reader
df2[df2$mean_duration == min(df2$mean_duration), ] # fastest reader
```

The following density plot shows the mean reading times for each participant split between L1 and L2.
```{r density}
df2 <- unique(df2)

ggplot(df2, aes(x = mean_duration, fill = L1)) +
  geom_density(alpha = 0.5) +
  labs(title = "Mean Reading Times between L1 and L2",
       x = "Duration",
       y = "Density") +
  theme_minimal()
```


## Correlation
**Note:** This uses the AAPz values. In this case, it must be determined how to deal with stopwords (see above).
```{r correlation}
cat("Correlation", "\n","Word length:", cor(word_length,duration), "\n","Word frequency:", cor(word_frequency, duration), "\n","Valency:", cor(valency, duration))
```

```{r correlation_by_participant}
df3 <- article %>%
  group_by(participant_id) %>%
  mutate(cor_valency=cor(AAPz, dur), cor_length=cor(word_length, dur), cor_frequency=cor(frequency, dur))

print(df3)
```

```{r corr_density}
ggplot(df3, aes(x = cor_valency, fill = L1)) +
  geom_density(alpha = 0.5) +
  labs(title = "Correlation between Valency and Duration by L1 and L2",
       x = "Correlation",
       y = "Density") +
  theme_minimal()
```

```{r correlation_by_language}
article %>%
  group_by(lang) %>%
  summarise(cor_valency=cor(AAPz, dur), cor_length=cor(word_length, dur), cor_frequency=cor(frequency, dur))
```

```{r}
ggplot(df3, aes(x = lang, y = cor_valency, fill = lang)) +
  geom_boxplot() +
  scale_fill_manual(values = terrain.colors(9)) +
  labs(title = "Boxplot of Correlation between Valency and Duration by Language",
       x = "Language",
       y = "Correlation")
```

Some exploratory median correlations:
```{r median_correlation}
cor(article$median_he, article$word_length)
cor(article$median_en, article$word_length)

cor(article$median_he, article$AAPz)
cor(article$median_en, article$AAPz)

cor(article$AAPz, article$frequency)
cor(article$AAPz, article$word_length)

summary(lm(median_ru ~ word_length + word_frequency + punctuation + valency, data = article))
```

## Regression
multifactorial linear regression analysis 
- step-down and step-up analysis (also interactions between variables)
- begin with step-down 
- draw up a maximum model based on what is expected from the literature, 1 code per RQ 

```{r RQ1}
summary(lm(duration ~ word_length + word_frequency + punctuation + valency + exposure_EN + 
             cft20 + lextale + motiv + start_learning, data = article))

summary(lm(duration ~ word_length + word_frequency + punctuation + valency + exposure_EN + 
             cft20 + lextale + motiv + start_learning, data = article))
```

```{r RQ2}
summary(lm(duration ~ word_length + word_frequency + punctuation + valency + 
             exposure_EN + cft20 + lextale + motiv + start_learning + lang + valency:lang, data = article))
```

```{r median_between}
median_duration <- article %>%
  group_by(token, tokenindex, L1) %>%
  summarise(valency = first(AAPz), medianDurAll = median(dur)) %>%
  pivot_wider(names_from = L1, values_from = medianDurAll)
```

```{r add_median}
article <- article %>%
  group_by(token, tokenindex, L1) %>%
  mutate(medianDurAll = median(dur), .after = dur)
```

```{r RQ3}
summary(lm(medianDurAll ~ word_length + word_frequency + punctuation + valency + exposure_EN + 
             cft20 + lextale + motiv + start_learning, data = article))

summary(lm(medianDurAll ~ L1 + word_length + word_frequency + punctuation + valency + valency:L1, data = article))
```

```{r feature_weights}
m <- lm(medianDurAll ~ word_length + word_frequency + punctuation + valency + exposure_EN + 
          cft20 + lextale + motiv + start_learning, data = article)

drop1(m, test = "F")
```


# Linear Mixed-Models
Comment: Would need ANOVA to see which model is more significant
```{r eval=FALSE, include=FALSE}
summary(lmer(log_duration ~ valency + (1 + valency | word_length)))
summary(lmer(log_duration ~ valency + (1 + valency | participant)))
summary(lmer(log_duration ~ valency + (1 + valency | participant) + (1 | word_length)))
```
