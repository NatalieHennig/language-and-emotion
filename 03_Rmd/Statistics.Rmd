# Statistical Exploration

**Libraries**
```{r message=FALSE, warning=FALSE}
library(lme4)
library(ggplot2)
library(hrbrthemes)
library(reshape2)
library(dplyr)
library(tidyr)
library(readr)
library(here)
library(lmerTest)
```

## Dataset
```{r load_data, message=FALSE}
RT_data <- read_csv(here("05_output", "reading_times2.csv"), col_names=TRUE, id=NULL)
```

```{r participant_count}
aggregate(participant_id ~ lang, data = RT_data, FUN = function(x) length(unique(x)))
```

```{r total_participants}
length(unique(RT_data$participant_id))
```

## Declare variables
```{r select_textnr}
#article <- RT_data %>% filter(textnr == 1)
article <- RT_data
```
"As regards very long fixations, deletion (but not merger) is also common. Eye-tracking researchers generally remove fixations longer than 800 ms, as recommended by Conklin and Pellicer-SÃ¡nchez (2016) and Keating (2014), because these fixations are believed to signal a lapse in attention." (Godfroid 2020: 261)
```{r outlier, eval=FALSE, include=FALSE}
# Cap the values in the dur column to a maximum of 800
#article$dur <- pmin(article$dur, 800)

# Remove duration over 800
#article <- article[article$dur <= 800, ]
```

### Log-Transformation
```{r log_duration}
article <- article %>% mutate(log_dur = log(dur)) # log transform
hist(article$dur)
hist(article$log_dur)
```

**Note:** Values that are *NA* must be dealt with. There are multiple options for handling missing values. 
While they could be removed, this would lead to a loss of data. Another approach is to set it to a very low value, for example 1ms.
```{r dependent_var}
article$dur[is.na(article$dur)] <- 0 
article$log_dur[is.na(article$log_dur)] <- 0

duration = article$dur
log_duration = article$log_dur
```

There are also AAPz scores for stopwords. The following code snippet changes all AAPz scores for stopwords to 0.
```{r independent_var}
# Update the AAPz column where is_stopword is 1
article$AAPz[article$is_stopword == 1] <- 0

valency = article$AAPz
```

```{r confounding_var}
# lexical-level 
word_length = article$word_length
word_frequency = article$frequency
article <- article %>% mutate(log_freq = log(frequency))
#predictability = TBD
POS = article$pos

# participant-level
participant = article$participant_id
```

How do I get bigrams?
```{r surprisal, eval=FALSE, include=FALSE}
bnc_size = 96928173 # total frequencies from the prepared file (bnc whole) without punctuation

article <- article %>% 
  mutate(
    word = frequency,
    previous_word = lag(frequency),
    conditional_prob = 0,
    bigram_surprisal = log(1 / (word / bnc_size)) + log(1 / conditional_prob)
  )
```

```{r adjust_column}
article <- article %>% mutate(L1 = ifelse(lang == "en", "L1", "L2"))
article$exposure_EN[article$lang == "en"] <- 100 # set exposure for EN participants
```

```{r organize_columns}
article <- article %>% select(textnr, lang, L1, participant_id, 
                              tokenindex, token,
                              skip, pos, is_stopword,
                              dur, log_dur,
                              AAPz, anew_score, vader_score,
                              punctuation, word_length, frequency, log_freq,
                              motiv, lextale, cft20, 
                              start_learning,
                              exposure_EN
                              )

# fluent, reading, reading_fluent, exposure_L1,
# country, family, `school/workplace`,
# spelling, TOWRE_word, TOWRE_nonword, vocab, vocab.t2.5
```

## Valency
```{r categorical_valency}
# Turn into categorical
positive = 0.1
negative = -0.1

article <- article %>%
  mutate(AAPz_cat = case_when(
    AAPz > positive ~ "Positive",
    AAPz < negative ~ "Negative",
    TRUE ~ "Neutral"), .after = AAPz)
```

```{r}
article %>%
    group_by(AAPz_cat, lang) %>%
    summarise(val = median(dur), .groups = "drop") %>% 
  pivot_wider(names_from = AAPz_cat, values_from = val)
```

```{r}
article %>%
    group_by(AAPz_cat, lang) %>%
    summarise(val = median(dur), .groups = "drop") %>%
  ggplot(aes(x = AAPz_cat, y = val, color = lang, group = lang)) +
  geom_point() +
  geom_line() +
  #facet_wrap(~lang) +
  labs(x = "Valency", y = "Median Duration", color = "Language") +
  theme_minimal()
```
**Important:** This doesn't say much about the reading times with respect to valency because other variables (such as word length) need to be considered as well.

## Means and Medians 
#### Mean and median across all readers for each word
```{r calc_all}
article %>%
  group_by(token, tokenindex, textnr) %>%
  summarise(mean_dur = round(mean(dur)), median_dur = round(median(dur)), .groups = 'drop')

article <- article %>%
    group_by(token, tokenindex, textnr) %>%
    mutate(mean_dur = mean(dur), median_dur = median(dur), .after=log_dur)
```

#### Median across language group for each word
Median duration of each token for each language group.
```{r calc_language}
article %>%
  group_by(textnr, tokenindex, token, lang) %>%
  summarise(medianDur = median(dur), .groups = 'drop') %>%
  spread(lang, medianDur, sep = "_") %>%
  rename_with(~gsub("^lang_", "median_", .), starts_with("lang_"))

article <- article %>%
  group_by(textnr, token, tokenindex, lang) %>%
  mutate(median_lang = median(dur), .after = median_dur)
```

#### Median across L1 and L2 for each word
```{r median_L1L2}
article %>%
  group_by(textnr, token, tokenindex, L1) %>%
  summarise(valency = first(AAPz), median_dur = median(dur), .groups = "drop") %>%
  pivot_wider(names_from = L1, values_from = median_dur)

article <- article %>%
  group_by(textnr, token, tokenindex, L1) %>%
  mutate(median_L1L2 = median(dur), mean_L1L2 = mean(dur), .after = median_dur)
```
## Descriptive Statistics
### Participants
```{r mean_participant}
participant_mean <- article %>% 
  group_by(participant_id) %>% 
  reframe(mean_duration = mean(dur), L1) %>%
  distinct()
```

```{r}
cat("Slowest reader: ", 
    participant_mean$participant_id[participant_mean$mean_duration == max(participant_mean$mean_duration)], ",",
    round(max(participant_mean$mean_duration)),"ms", "\n")

cat("Fastest reader: ", 
    participant_mean$participant_id[participant_mean$mean_duration == min(participant_mean$mean_duration)], ",",
    round(min(participant_mean$mean_duration)), "ms")
```

The following density plot shows the mean reading times for each participant split between L1 and L2.
```{r density}
L1L2_mean <- participant_mean %>%
  group_by(L1) %>%
  summarize(L1L2_mean_dur = mean(mean_duration))

ggplot(participant_mean, aes(x = mean_duration, fill = L1)) +
  geom_density(alpha = 0.75) +
  geom_vline(data=L1L2_mean, aes(xintercept=L1L2_mean_dur, linetype = L1)) +
  labs(title = "Mean Reading Times between L1 and L2",
       x = "Duration",
       y = "Density") +
  scale_fill_grey() +
  theme_minimal()
```

### Language Group
#### Duration 
```{r mean_lang}
article %>% group_by(lang) %>% 
  summarize(mean_duration = round(mean(dur)), 
            median_duration = median(dur), 
            standard_deviation = round(sd(dur)))
```

```{r percentage_0}
cat("Percentage of 0ms duration (skipped words) using",
    "\n the mean: ",(sum(article$mean_dur==0)/length(article$mean_dur))*100,
    "\n the median: ", round((sum(article$median_dur==0)/length(article$median_dur))*100, 2),
    "\n the median between L1 and L2: ",(round(sum(article$median_L1L2==0)/length(article$median_L1L2)*100, 2)),
    "\n overall: ",round((sum(article$dur==0)/length(article$dur))*100, 2))
```
**important:** Median contains a lot of 0! Mean performs better! However, the disadvantage of the mean is that the durations are much higher overall.

```{r percentage_0_by_lang}
article %>%
  group_by(lang) %>%
  summarise(zero_dur = round((sum(dur == 0) / n()) * 100, 2),
            zero_dur_lang = round((sum(median_lang == 0) / n()) * 100, 2))
```


```{r plot_1}
ggplot(article, aes(x = lang, y = median_lang, fill = lang)) +
  geom_boxplot() +
  scale_y_continuous(limits = c(0, 1000)) +
  scale_fill_manual(values = terrain.colors(9)) +
  labs(title = "Boxplot of Duration by Language",
       x = "Language",
       y = "Duration")
```

## Correlation
**Note:** This uses the AAPz values. In this case, it must be determined how to deal with stopwords (see above).
```{r correlation}
cat("Correlation", "\n","Word length:", cor(word_length,duration), "\n","Word frequency:", cor(word_frequency, duration), "\n","Valency:", cor(valency, duration))
```

```{r correlation_by_language}
article %>%
  group_by(lang) %>%
  summarise(cor_valency=cor(AAPz, dur), cor_length=cor(word_length, dur), cor_frequency=cor(frequency, dur))
```

```{r correlation_by_L1L2}
part_correlation <- article %>%
  group_by(participant_id) %>%
  mutate(cor_valency=cor(AAPz, dur), cor_length=cor(word_length, dur), cor_frequency=cor(frequency, dur)) %>%
  group_by(L1) %>%
  mutate(mean_cor_valency = mean(cor_valency)) %>%
  reframe(L1, participant_id, lang, cor_valency, cor_length, cor_frequency, mean_cor_valency)
```

```{r corr_density}
ggplot(part_correlation, aes(x = cor_valency, fill = L1)) +
  geom_density(alpha = 0.75) +
  geom_vline(aes(xintercept = mean_cor_valency, linetype = L1)) +
  labs(title = "Correlation between Valency and Duration by L1 and L2",
       x = "Correlation",
       y = "Density") +
  scale_fill_grey() +
  theme_minimal()
```

```{r boxplot_corr_lang}
ggplot(part_correlation, aes(x = lang, y = cor_valency, fill = lang)) +
  geom_boxplot() +
  scale_fill_manual(values = terrain.colors(9)) +
  labs(title = "Boxplot of Correlation between Valency and Duration by Language",
       x = "Language",
       y = "Correlation")
```

## Linear Regression
Multifactorial linear regression analysis. Step-down and step-up analysis, including interactions between variables.

```{r factors}
article$L1 <- factor(article$L1)
article$punctuation <- factor(article$punctuation)
article$is_stopword <- factor(article$is_stopword)
```


### RQ1
When taking the median of all, *exposure_EN + cft20 + lextale + motiv + start_learning* doesn't make sense to be included. 
**Question:** why is the estimate significant even when ANEW or VADER is used for valency? -> Because the measures are repeated for each participant. This needs to be changed.
```{r RQ1_data}
RQ1_df <- article %>%
  group_by(textnr, token, tokenindex) %>%
  select(textnr, tokenindex, token, mean_dur, median_dur, AAPz, AAPz_cat, anew_score, punctuation, word_length, frequency, log_freq, is_stopword) %>%
  distinct()
```

```{r RQ1} 
m1.1 <- lm(median_dur ~ word_length + frequency + punctuation + AAPz + is_stopword, data = RQ1_df)
m1.2 <- lm(median_dur ~ word_length + log_freq + punctuation + AAPz + is_stopword , data = RQ1_df)

# polynomial
m1.3 <- lm(median_dur ~ word_length + frequency + punctuation + AAPz + I(AAPz^2) + is_stopword, data = RQ1_df)
m1.4 <- lm(median_dur ~ word_length + log_freq + punctuation + AAPz + I(AAPz^2) + is_stopword, data = RQ1_df)

summary(m1.1)
summary(m1.2)
summary(m1.3)
summary(m1.4)

summary(lm(mean_dur ~ word_length + log_freq + punctuation + AAPz + I(AAPz^2) + is_stopword + log_freq:(AAPz + I(AAPz^2)) + punctuation:(AAPz + I(AAPz^2)), data = RQ1_df))
```

### RQ2
```{r RQ2_df}
RQ2_df <- article %>%
  group_by(textnr, token, tokenindex) %>%
  select(textnr, tokenindex, token, L1, median_lang, lang, AAPz, AAPz_cat, anew_score, punctuation, word_length, frequency, log_freq, is_stopword) %>%
  distinct()

RQ2_ru <- RQ2_df[RQ2_df$lang == 'ru',]
RQ2_he <- RQ2_df[RQ2_df$lang == 'he',]
RQ2_fi <- RQ2_df[RQ2_df$lang == 'fi',]
```

```{r RQ2}
RQ2.1 <- lm(median_lang ~ word_length + log_freq + punctuation + AAPz + I(AAPz^2) + is_stopword + log_freq:(AAPz + I(AAPz^2)) + punctuation:(AAPz + I(AAPz^2)), data = RQ2_ru)
RQ2.2 <- lm(median_lang ~ word_length + log_freq + punctuation + AAPz + I(AAPz^2) + is_stopword + log_freq:(AAPz + I(AAPz^2)) + punctuation:(AAPz + I(AAPz^2)), data = RQ2_he)
RQ2.3 <- lm(median_lang ~ word_length + log_freq + punctuation + AAPz + I(AAPz^2) + is_stopword + log_freq:(AAPz + I(AAPz^2)) + punctuation:(AAPz + I(AAPz^2)), data = RQ2_fi)

summary(RQ2.1)
summary(RQ2.2)
summary(RQ2.3)
```


```{r RQ2_cat, eval=FALSE, include=FALSE}
#article$AAPz_cat <- factor(article$AAPz_cat)
#article$AAPz_cat <- relevel(article$AAPz_cat, ref = "Neutral")

summary(lm(median_lang ~ word_length + word_frequency + punctuation 
           + AAPz_cat:lang
           + AAPz_cat:word_length + AAPz_cat:word_frequency + AAPz_cat:punctuation, data = article)) 
```

**Note:** Do the variables *exposure_EN + cft20 + lextale + motiv + start_learning* need to be medians between language groups?
```{r RQ2_df_median, eval=FALSE, include=FALSE}
RQ2_df <- article %>% group_by(lang, textnr, tokenindex, token) %>% 
  reframe(median_lang, word_length, frequency, punctuation, AAPz, 
          mean_exposure = round(mean(exposure_EN)), 
          mean_cft = round(mean(cft20)), 
          mean_lextale = round(mean(lextale)), 
          mean_motiv = round(mean(motiv)), 
          mean_start = round(mean(start_learning))) %>%
  distinct()
```
why is *ee* mean_start *NA*? There are some missing values, these participants will be excluded.

```{r eval=FALSE, include=FALSE}
summary(lm(median_lang ~ word_length + word_frequency + punctuation + valency + 
             exposure_EN + cft20 + lextale + motiv + start_learning 
           + lang + valency:lang, data = article)) 

summary(lm(median_lang ~ word_length + frequency + punctuation + AAPz + 
             mean_exposure + mean_cft + mean_lextale + mean_motiv + mean_start 
           + lang + AAPz:lang, data = RQ2_df)) 

summary(lm(median_lang ~ word_length + frequency + punctuation + AAPz + 
             mean_exposure + mean_cft + mean_lextale + AAPz:lang,
             data = RQ2_df)) 
```
**Question**: Why are some languages not included?


### RQ3
```{r RQ3_data}
RQ3_df <- article %>%
  group_by(textnr, token, tokenindex) %>%
  select(textnr, tokenindex, token, L1, median_L1L2, mean_L1L2, AAPz, AAPz_cat, anew_score, punctuation, word_length, frequency, log_freq, is_stopword) %>%
  distinct()
```

```{r RQ3}
summary(lm(median_L1L2 ~ word_length + frequency + punctuation +
             L1 + AAPz + AAPz:L1, data = RQ3_df))
```
```{r polynomial}
summary(lm(median_L1L2 ~ word_length + frequency + punctuation +
             L1 + AAPz + I(AAPz^2), RQ3_df))
```
```{r feature_weights}
RQ3 <- lm(median_L1L2 ~ word_length + frequency + punctuation + AAPz + L1, data = RQ3_df)

drop1(RQ3, test = "F")
```

```{r RQ3_dfs}
# Sort by column L1
RQ3_df <- RQ3_df[order(RQ3_df$L1), ]

# Split the data frame into two tables
L1_RQ3 <- RQ3_df[1:(nrow(RQ3_df) / 2), ]
L2_RQ3 <- RQ3_df[((nrow(RQ3_df) / 2) + 1):nrow(RQ3_df), ]
```

```{r L1_RQ3}
RQ3_model1 <- lm(mean_L1L2 ~ word_length + log_freq + punctuation + AAPz + I(AAPz^2) + is_stopword + log_freq:(AAPz + I(AAPz^2)) + punctuation:(AAPz + I(AAPz^2)), data = L1_RQ3)
summary(RQ3_model1)
```
```{r L2_RQ3}
RQ3_model2 <- lm(mean_L1L2 ~ word_length + log_freq + punctuation + AAPz + I(AAPz^2) + is_stopword + log_freq:(AAPz + I(AAPz^2))+  log_freq:(AAPz + I(AAPz^2)) + punctuation:(AAPz + I(AAPz^2)), data = L2_RQ3)
summary(RQ3_model2)
```

## Linear Mixed-Models
Comment: Would need ANOVA to see which model is more significant
```{r}
summary(lmer(dur ~ word_length + log_freq + punctuation + AAPz + I(AAPz^2) + is_stopword + log_freq:(AAPz + I(AAPz^2)) + punctuation:(AAPz + I(AAPz^2))
             + (1 | token / textnr) + (1 | lang / participant), article))
```
