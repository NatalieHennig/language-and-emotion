# Statistical Exploration

## Libraries
```{r message=FALSE, warning=FALSE}
library(lme4)
library(ggplot2)
library(reshape2)
```

## Declare variables
```{r variables}
# data
article <- text1 %>% select(-c(nfix, refix))

# dependent variable
duration = article$dur
log_duration = log(duration)

# independent variable
valency = article$AAPz

# confounders
# lexical-level 
word_length = article$word_length
word_frequency = article$frequency
#predictability = TBD
POS = article$pos

# participant-level
participant = article$participant_id
```

```{r concatenate_tables}
text1_to_4 <- rbind(text1, text2, text3, text4)
```


### Means and Medians 
#### Across all readers
```{r means_all}
text1$dur[is.na(text1$dur)] <- 0
text1 %>%
  group_by(token, tokenindex) %>%
  summarise(meanDurAll = mean(dur), medianDurAll = median(dur))
```

```{r}
text1 %>%
    group_by(tokenindex, lang, token) %>%
    summarise(medianDur = median(dur)) %>%
    spread(lang, medianDur, sep = "_")
```


## Correlation
**Note:** This uses the AAPz values. In this case, the stopwords also contain valency scores due to the AAPz. 
TODO: What is the correlation when the valency score for stopwords is set to 0?

```{r correlation}
duration[is.na(duration)] <- 0

cor(word_length,duration)
cor(word_frequency, duration)
cor(valency, duration)
```

with article set to **text1_to_4**
```
group_by(participant) %>%
+     summarize(cor=cor(valency, duration))
Error in UseMethod("group_by") : 
  no applicable method for 'group_by' applied to an object of class "factor"
```

```{r correlation_by_participant}
article$dur[is.na(article$dur)] <- 0

article %>%
  group_by(participant_id) %>%
  summarise(cor_valency=cor(AAPz, dur), cor_length=cor(word_length, dur), cor_frequency=cor(frequency, dur))
```
```{r correlation_by_language}
 cor_by_language <- article %>%
  group_by(lang) %>%
  summarise(cor_valency=cor(AAPz, dur), cor_length=cor(word_length, dur), cor_frequency=cor(frequency, dur))

print(cor_by_language)
```

```{r cor_heatmap}
# Melt the data for easier plotting
melted_data <- melt(cor_by_language, id.vars = "lang")

# Use the absolute values of correlations
melted_data$value <- abs(melted_data$value)

heatmap_plot <- ggplot(melted_data, aes(x = variable, y = lang, fill = value)) +
    geom_tile(color = "white") +
    scale_fill_gradient2(low = "red", mid = "yellow", high = "blue", midpoint = 0, guide = "colorbar") +
    labs(title = "Correlation Heatmap",
         x = "Variables",
         y = "Language Groups") +
    theme_minimal()

print(heatmap_plot)
```


### Log-Transformation
Note: Values that are *NA* must be dealt with. There are multiple options for handling missing values. 
While they could be removed, this would lead to a loss of data.  
Another approach is to set it to a very low value, for example 1ms.
```{r}
hist(duration)
hist(log_duration)
```

## Regression
COMMENT: multifactorial linear regression analysis (multiple factors - see which one is significant)
- step-down and step-up analysis (also interactions between variables)
- begin with step-down 
- draw up a maximum model based on what is expected from the literature, 1 code per RQ -> send per email, then weed out the insignificant variables
```{r}
lm(duration ~ valency)
```


```{r log_transform}
# Replace Inf and -Inf values with 0
log_duration[is.infinite(log_duration)] <- 0

result <- lm(log_duration ~ valency)
summary(result)
```

Comment: Would need ANOVA to see which model is more significant
```{r}
result <- lmer(log_duration ~ valency + (1 + valency | word_length))
summary(result)
```

```{r}
result <- lmer(log_duration ~ valency + (1 + valency | participant))
summary(result)
```

```{r}
result <- lmer(log_duration ~ valency + (1 + valency | participant) + (1 | word_length))
summary(result)
```



