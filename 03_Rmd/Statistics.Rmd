# Statistical Exploration

## Libraries
```{r message=FALSE, warning=FALSE}
library(lme4)
library(ggplot2)
library(reshape2)
library(dplyr)
library(tidyr)
library(readr)
library(here)
```


```{r load_data, message=FALSE}
RT_data <- read_csv(here("05_output", "reading_times.csv"), col_names=TRUE, id=NULL)
```

```{r participant_count}
aggregate(participant_id ~ lang, data = RT_data, FUN = function(x) length(unique(x)))
length(unique(RT_data$participant_id))
```


## Declare variables
```{r select_textnr}
article <- RT_data %>% filter(textnr == 1)
#article <- RT_data
```

```{r dependent_var}
article <- article %>% mutate(log_dur = log(dur)) # log transform

duration = article$dur
log_duration = article$log_dur

article$dur[is.na(article$dur)] <- 0 # set NA to 0 
article$log_dur[is.na(article$log_dur)] <- 0
```

```{r independent_var}
valency = article$AAPz
```

```{r confounding_var}
# lexical-level 
word_length = article$word_length
word_frequency = article$frequency
#predictability = TBD
POS = article$pos

# participant-level
participant = article$participant_id
```


### Log-Transformation
Note: Values that are *NA* must be dealt with. There are multiple options for handling missing values. 
While they could be removed, this would lead to a loss of data.  
Another approach is to set it to a very low value, for example 1ms.
```{r}
hist(duration)
hist(log_duration)
```


### Means and Medians 
#### Across all readers
```{r calc_all}
article %>%
  group_by(token, tokenindex) %>%
  summarise(meanDurAll = mean(dur), medianDurAll = median(dur))
```

Median duration of each token for each language group-
```{r calc_language}
median_duration <- article %>%
    group_by(tokenindex, lang, token) %>%
    summarise(medianDur = median(dur)) %>%
    spread(lang, medianDur, sep = "_")
```

```{r join_results}
article <- left_join(article, median_duration, by = c("token", "tokenindex"))
rm(median_duration)
```

## Correlation
**Note:** This uses the AAPz values. In this case, the stopwords also contain valency scores due to the AAPz. 
TODO: What is the correlation when the valency score for stopwords is set to 0?

```{r correlation}
duration[is.na(duration)] <- 0

cor(word_length,duration)
cor(word_frequency, duration)
cor(valency, duration)
```

with article set to **reading_time**
```
group_by(participant) %>%
+     summarize(cor=cor(valency, duration))
Error in UseMethod("group_by") : 
  no applicable method for 'group_by' applied to an object of class "factor"
```

```{r correlation_by_participant}
article %>%
  group_by(participant_id) %>%
  summarise(cor_valency=cor(AAPz, dur), cor_length=cor(word_length, dur), cor_frequency=cor(frequency, dur))
```
```{r correlation_by_language}
article %>%
  group_by(lang) %>%
  summarise(cor_valency=cor(AAPz, dur), cor_length=cor(word_length, dur), cor_frequency=cor(frequency, dur))
```


Some exploratory median correlations:
```{r median_correlation}
cor(article$lang_he, article$word_length)
cor(article$lang_en, article$word_length)

cor(article$lang_he, article$AAPz)
cor(article$lang_en, article$AAPz)

cor(article$lang_he, article$frequency)
cor(article$lang_en, article$frequency)

cor(article$AAPz, article$frequency)
cor(article$AAPz, article$frequency)

cor(article$AAPz, article$word_length)
cor(article$AAPz, article$word_length)

summary(lm(lang_ru ~ word_length + word_frequency + punctuation + valency, data = article))
```

## Regression
COMMENT: multifactorial linear regression analysis (multiple factors - see which one is significant)
- step-down and step-up analysis (also interactions between variables)
- begin with step-down 
- draw up a maximum model based on what is expected from the literature, 1 code per RQ -> send per email, then weed out the insignificant variables
```{r}
summary(lm(duration ~ valency))

summary(lm(log_duration ~ valency))
```
log-transformed duration:
- R^2 is 2.8% -> article 1 to 4
- R^2 is 3.4% -> article 1

# Linear Mixed-Models
Comment: Would need ANOVA to see which model is more significant
```{r eval=FALSE, include=FALSE}
summary(lmer(log_duration ~ valency + (1 + valency | word_length)))
summary(lmer(log_duration ~ valency + (1 + valency | participant)))
summary(lmer(log_duration ~ valency + (1 + valency | participant) + (1 | word_length)))
```
