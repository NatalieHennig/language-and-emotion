---
title: "Preprocessing"
author: "Natalie-Tieda Hennig"
date: "2023-11-10"
output: 
  html_document:
  toc: true
  toc_float: true
  code_folding: hide
---

# Setup
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

### Define path
- *here* shows the root file - no need to manually set the working directory, which increases reproducibility.

```{r path}
here::i_am("03_Rmd/Preprocessing.Rmd")
library(here)
```

### Libraries
- *dplyr*: table manipulation
- *reticulate*: interoperability between Python and R
- *quanteda* and *quanteda.sentiment*: statistical text operations, ANEW ratings
- *tidyr*: table manipulation
- *tm*: text mining - removing punctuation etc.
- *stringr*: to manipulate strings
- *readxl*: to read excel files

```{r libraries, message=FALSE, warning=FALSE, results="hide"}
library(dplyr)
library(reticulate)
library(quanteda)
library(quanteda.sentiment)
library(tidyr)
library(spacyr)
library(tm)
library(stringr)
library(readxl)
library(readr)
library(tidyverse)
use_condaenv("thesis23")
```
**Note** Created a conda environment within the language-and-emotion project file *./thesis23*.
Installing packages using the following command (example *nltk*): *py_install("nltk", "./thesis23")*
To list the downloaded packages use *py_list_packages()*

### Load the data
```{r main_data, results="hide"}
load(here("01_data", "meco L2", "primary data","eye tracking data","joint_data_l2_trimmed.rda"))
```

# Preprocessing
### Create table
1.  Define which columns are relevant. The relevant columns will be used to create a concise table for subsequent analysis.
*Subid* is not needed, because the leap-q files contain a *uniform_id*, which acts as a key. 
```{r select_columns}
eye_table <- joint.data %>% select(uniform_id, lang, itemid, ianum, ia, skip, nfix, refix, dur, firstrun.dur, firstrun.gopast, firstrun.gopast.sel, firstfix.dur)
```

2. Rename the columns for better understanding. 
```{r rename_columns}
eye_table <- eye_table %>% rename("participant_id" = "uniform_id", "textnr" = "itemid", "tokenindex" = "ianum", "token"="ia") 
```

3. Sort by token index, such that the words in the article are ordered. 
```{r ordered_words}
eye_table <- eye_table %>% group_by(textnr, participant_id) %>% arrange(tokenindex, .by_group = TRUE) %>% distinct()
```

#### Punctuation
Punctuation is removed and replaced by a binary column that indicates whether there was punctuation with a word or not - in the same way as Schneider (2023: 164).
```{r punctuation}
eye_table <- eye_table %>% mutate(punctuation = ifelse(str_detect(token, "[[:punct:]]"), 1, 0))
eye_table$token = removePunctuation(eye_table$token) 
```

#### Word Length
The word length is calculated by the number of characters.
```{r token_length}
eye_table$word_length = nchar(eye_table$token)
```

#### Add POS-Tags
spacy_parse *(spaCy Version: 3.4.1, language model: en_core_web_sm)* is used to add POS tags to the words. 
It contains both a detailed set *tags*, as well as a less fine-grained universal set *pos*.The POS tags are based on the **Penn Treebank** tag set.
```{r parsing, eval=FALSE, message=FALSE, include=FALSE, results='hide'}
#spacy_install(envname = "./thesis23/bin/python", python_version = "3.10")
spacy_initialize()

POS = spacy_parse(eye_table$token, tag=TRUE)
POS <- POS %>% select(token, pos, tag, lemma)

POS <- semi_join(POS, eye_table, by = 'token')
write.csv(POS, here("05_output", "pos_tags.csv"))

#merged_data <- merge(eye_table, x, by = 'token')
```
For some reasons, after adding the POS with spacey, the dataset contained 657'993 rows instead of 656'000. 
After comparing the differences, it appeared that there were multiple empty rows that were labeled **PUNCT**. These rows were discarded.

Code: 
```
> eye_table2 = eye_table
> result <- semi_join(POS, eye_table2, by = 'token')
> not_equal_rows <- anti_join(POS, result, by = 'token')
```
Output **not_equal_rows**:
```
token | pos   | tag
“     | PUNCT | ``
```

```{r add_pos_to_main}
POS <- read.csv(here("05_output", "pos_tags.csv"))

eye_table$pos <- POS$pos
eye_table$tag <- POS$tag
eye_table$lemma <- POS$lemma
```

#### Word Frequency
Frequencies are from the BNC. The BNC CQP-Web version contains 98,313,429 tokens. Only tokens with frequencies >1 are extracted. 
```{r get_frequency, message=FALSE}
# POS were mapped using a python script (CLAWS to Penn)
frequency_whole <- read.csv(here("05_output", "mapped_pos_frequency.csv"))

frequency_whole <- frequency_whole %>% group_by(Word, Tag) %>%
    summarise(frequency = sum(Frequency))
```
Duplicates are summed because there are multiple tags such as *AJ0* and *AJ0-NN1*, which both are changed to *ADJ*.

```{r add_frequency}
eye_table <- eye_table %>% mutate(token.lower = tolower(token))

eye_table <- left_join(eye_table, frequency_whole, by = c("token.lower" = "Word", "pos" = "Tag")) %>%
  mutate(frequency = replace(frequency, is.na(frequency), 0))
```

```{r extract_unmatched_frequencies}
# Missing frequencies for the words and pos combination in eye_table
missing_frequency <- subset(eye_table, frequency == 0)
missing_frequency <- unique(missing_frequency[, c("token.lower", "pos")])

# Words that exist in the frequency list
not_matched_token = unique(eye_table$token.lower[which(eye_table$frequency == 0)])
not_matched <- subset(frequency_whole, frequency_whole$Word %in% not_matched_token)
```
Possibilities:
- **Case 1:** There exist multiple entries in *eye_table* with different tags for the same token
- **Case 2:** A token in the *eye_table* does not exist in the frequency list
- **Case 3:** Tags from the frequency table diverge from the ones in *eye_table* 

```{r case_1}
length(missing_frequency$token.lower)
length(unique(missing_frequency$token.lower))
```
Only *rainforest* occurs twice but it can be treated as a noun:
- rainforest PROPN
- rainforest X

```{r case_2}
# Number of tokens not in the frequency list
length(unique(missing_frequency$token.lower)) - length(unique(not_matched$Word))

setdiff(unique(missing_frequency$token.lower), unique(not_matched$Word))
```

```{r case_3}
# Drop the Tag column
not_matched <- select(not_matched, -c(Tag))

# Combine without tags
eye_table <- left_join(eye_table, not_matched, by = c("token.lower" = "Word")) 
```

```{r consolidate_columns}
eye_table$frequency <- rowSums(eye_table[, c("frequency.x", "frequency.y")], na.rm = TRUE)

# Remove frequency.x and frequency.y 
eye_table <- eye_table[, !(names(eye_table) %in% c("frequency.x", "frequency.y"))]
```

```{r calculate_outcome}
missing_frequency <- subset(eye_table, frequency == 0)
missing_frequency <- unique(missing_frequency[, c("token.lower", "pos")])

cat("Percentage of missing frequency values:",  round(((sum(eye_table$frequency==0)/length(eye_table$frequency))*100), 2), "%\n")
#sum(eye_table$frequency==0)
#length(eye_table$frequency)
```

```{r remove_freq_dfs}
rm(frequency_whole, missing_frequency, not_matched)
```

#### Identify stopwords
```{r stopwords}
english_stopwords <- stopwords("en")

# Add a new column to indicate stopwords
eye_table$is_stopword <- as.integer(eye_table$token.lower %in% english_stopwords)
```

#### Ensure data quality
Count the number of words in each text to see if "ia" equals the token.
```{r quality_check_ia, eval=FALSE}
textlength <- eye_table %>% group_by(textnr, participant_id) %>% summarise(length=n(), .groups = "drop")
```

> ```unique(textlength$length)```

```
[1] 161 526 572 126 440 416 432 120 428 392 412 148 530 546 516  98 318 334 107 372 360 350 386 356 142 494 450 474 492 187 670 630 147 540 470 514 173 612 624 610 133 500 444 116 404 382 402
```

There are some duplicates in the dataset. For example, the participant *en_11* contains some rows multiple times which skews the numbers in *textlength*. 
These duplicates must be removed and seem to be an error in the original dataset *joint.data* from the MECO L2.
After adding *distinct()* to the *eye_table* in the chunk **ordered_words**, the duplicates were removed.

> ```x <- tibble(nr = textlength$textnr, tok = textlength$length) %>% distinct() %>% arrange(nr)```

```
# A tibble: 12 × 2
   nr      tok
   <chr> <int>
 1 1       161
 5 2        98
 6 3       107
 7 4       142
 8 5       187
 9 6       147
10 7       173
11 8       133
12 9       116
 2 10      126
 3 11      120
 4 12      148
```
Some texts contain more words than indicated: 

- text 5 should be 185 (=-2)
- text 9 should be 115 (=-1)
- text 12 should be 146 (=-2)

A closer look at the text shows that some words were split up, such as *anti-virus* in text 5, which was counted as two tokens *anti-* and *virus*. 
Since these words have individual measurements, they will be counted as separate words.

#### Split tables according to text number
```{r split_text_by_nr}
text_table <- function(nr){
  return(subset(eye_table, subset = textnr == nr))
}
```

```{r text_by_nr}
text1 <- text_table(1)
text2 <- text_table(2)
text3 <- text_table(3)
text4 <- text_table(4)
text5 <- text_table(5)
text6 <- text_table(6)
text7 <- text_table(7)
text8 <- text_table(8)
text9 <- text_table(9)
text10 <- text_table(10)
text11 <- text_table(11)
text12 <- text_table(12)
```

### Add valency scores to each text
There exist several valency scores. Hutto & Gilbert (2014) provide a rundown of currently used polarity and semantic intensity lexicons.

- Human word ratings
  - ANEW
- Computational (https://doi.org/10.1075/ssol.18002.jac)
  - VADER 
  - SentiArt

First, the columns *textnr*, *tokenindex*, and *token* for each text is extracted. They serve as a key, so that they can be reassigned with the polarity scores to the main table. The output is saved as a csv file.
*distinct()* is used to get the full text only once, instead times the number of participants. It is currently set to *eval=FALSE* because it only needs to run once, to extract the csv files.
```{r export_text_column, eval=FALSE, include=FALSE}
text_outputs = 12
for (nr in 1:text_outputs){
  out <- get(paste0("text", nr)) %>% ungroup() %>% select(textnr, tokenindex, token) %>% distinct()
  write.csv(out, here("05_output", paste0(nr, "_text_tokens.csv")))
}
```

#### VADER
```{r call_vader, eval=FALSE, message=FALSE, warning=FALSE, include=FALSE}
source_python(here("04_scripts", "01_vader.py"))

for (nr in 1:text_outputs){
  add_vader_score(nr)
}
```
*assign_polarity_scores* has positive and negative values as optional argument. The default is set to *0.02*/*-0.02*. Files are saved in 05_output as *nr_vader_scores.csv*.
Problem: A brief glance at the assigned polarity scores shows that the texts are mostly neutral. This is due to the nature of the texts, i.e., the fact that encyclopedic style articles were chosen.

Attach the valency scores to the main tables.
```{r merge_vader}

for (i in 1:12) { 
  file_path <- here("05_output", "vader_scores", paste0(i, "_vader_scores.csv"))
  
  # Check if the file exists before attempting to read it
  if (file.exists(file_path)) {
    vader_scores <- read.csv(file_path)
    merger <- vader_scores %>% select(tokenindex, vader_score)
    
    text_name <- paste0("text", i)
    
    # Merge the data frame with its corresponding VADER scores
    text_df <- get(text_name)
    text_df <- merge(text_df, merger, by = c("tokenindex")) %>%
      group_by(textnr, participant_id) %>%
      arrange(tokenindex, .by_group = TRUE)
    
    # Update the original data frame
    assign(text_name, text_df, envir = .GlobalEnv)
  } else {
    cat("File", file_path, "does not exist.\n")
  }
}
```

#### ANEW
Valence ratings in which humans rated words with values for pleasure, dominance and arousal. 
- Based on Bradley and Lang (1999): Relatively small dataset of 1034 words. 
- Bradley and Lang (2009?): 2477 words.
- The quanteda.sentiment ANEW dataset contains 2741 words.
There exist several updated/extended versions, such as Warriner et al. (2013) word rating lexicon using Amazon Mechanical Turk, as well as an expansion technique, named ANEW+, as proposed by Shaikh et al. (2016).

```{r call_anew}
source(here("04_scripts","02_anew.R"), local = knitr::knit_global())
#lapply(valence(data_dictionary_ANEW), head)
#pleasure <- valence(data_dictionary_ANEW)[["pleasure"]]

texts_listed <- list("text1", "text2", "text3", "text4", "text5", "text6", "text7", "text8", "text9", "text10", "text11", "text12")

for (text_name in texts_listed) {
  add_anew_score(text_name)
}
```

#### SentiArt
"Words with high and low AAP values (e.g. “nature” versus “criminal”) are more likely to induce positive and negative emotions, respectively, and words with AAP values approaching zero are considered to be more neutral (e.g. “bottle”). AAP has been validated as an accurate predictor of self-reported valence ratings of single words, and has been found to predict emotional states during story reading better than valence ratings from an affective word database" (Jacobs & Kinder, 2019)" (Lei, Willems & Eekhof 2023: 999).

- Uses Vector Space Models instead of emotional dictionaries
- High predictive accuracy
- Pre-trained on the wiki.en corpus, particularly suitable for the texts under investigation
- expandable to different languages

"it was shown that the computational AAP values predicted ~2800 human database (subtlex). In a first cross-validation study, it was shown that the computational AAP values establishing the SDEWAC predicted ~2800 human valence ratings from the BAWL better than the computational valence values,VSM as the best-fitting of the three VSMs (R2 > 0.5, r = 0.72, p < 0.00012)." (Jacobs & Kinder 2019) 

```{r add_AAP}
senti_scores <- read_xlsx(here("01_data", "250kSentiArt_EN.xlsx"))
senti_scores <- senti_scores %>% select("word", "AAPz")

for (text_name in texts_listed) {
  assign(text_name, left_join(get(text_name), senti_scores, by = c("token" = "word")) %>% mutate(AAPz = ifelse(is.na(AAPz), 0, AAPz)))
}

```

#### Valency score coverage
Words that are not in the dictionary are automatically assigned a value of 0.00 which needs to be taken into consideration. Therefore, POS-Tags need to be added to compute the coverage for only the content words. The column **is_stopword** removes function words and non-content words. It is used to calculate the coverage of content words by making sure that tokens in which the valency scores are above 0.00 and do not contain stopwords are calculated, and divided by the number of rows minus the number of rows that contain stopwords.

```{r calculate coverage}
print("Coverage")

for (text_name in texts_listed) {
  text_in <- get(text_name)
  coverage_anew = sum(text_in$anew_score != 0.00 & text_in$is_stopword != 1) / (nrow(text_in) - sum(text_in$is_stopword == 1)) *100
  coverage_vader = sum(text_in$vader_score != 0.00 & text_in$is_stopword != 1) / (nrow(text_in) - sum(text_in$is_stopword == 1)) *100
  coverage_sentiart = sum(text_in$AAPz != 0.00 & text_in$is_stopword != 1) / (nrow(text_in) - sum(text_in$is_stopword == 1)) *100
  print(paste0(text_name, " - ", "ANEW: ", round(coverage_anew,0), "%, ", "VADER: ", round(coverage_vader,0), "%, ", "AAP: ", round(coverage_sentiart,0), "%"))
}
```
*Note*: The coverage increases for ANEW, when lemmas instead of tokens are used. This can't be done for VADER, as it influences the intensity while the coverage remains the same.

```{r remove_dfs}
rm("merger", "POS", "texts_listed", "vader_scores", "joint.data", "text_df", "text_in", "senti_scores")
```

# Overlap between texts
```{r get_overlap}
overlap <- unique(select(eye_table, c(participant_id, textnr)))

# Add column value with values set to 1
overlap <- overlap %>% mutate(value = 1)

overlap <- overlap %>%
  pivot_wider(names_from = textnr, values_from = value, values_fill = 0)
```

```{r participants_all_text}
all_texts <- overlap %>%
  filter(if_all(all_of(names(.)[-1]), ~. == 1))
```
There are only 89 participants who have read all 12 texts. In contrast, 251 participants have read texts 1,2, 3, and 4 (see below).

```{r count_texts_read}
# Count occurrences of "1" in each numeric column except the first one
overlap %>%
  select(-participant_id) %>%
  ungroup() %>%
  summarise(across(everything(), ~sum(. == 1, na.rm = TRUE)))
```
Texts 1, 2, 3, and 4 have over 400 participants.

```{r texts_read}
subset_overlap <- select(overlap, c("participant_id", "1", "2", "3", "4"))
subset_texts <- subset_overlap %>% filter(if_all(all_of(names(.)[-1]), ~. == 1))

# Create a new column for language
subset_texts <- subset_texts %>%
  mutate(language = substr(participant_id, 1, 2))

# Count participants for each language
subset_texts %>%
  group_by(language) %>%
  summarise(participant_count = n())
```
```{r remove_dfs}
rm(overlap, all_texts, subset_overlap)
```

# Individual Differences 
```{r load_id_files}
load(here("01_data", "meco L2", "primary data","comprehension data","joint_l2_acc_full_breakdown.rda"))
load(here("01_data", "meco L2", "primary data","individual differences data","joint.ind.diff.l2.rda"))

comprehension_raw <- comp.long
id_tests <- joint_id
leap_q <- read_xlsx(here("01_data", "Leap_Q_combined.xlsx"), sheet = 2)

rm(comp.long, joint_id)
```

## Comprehension
Calculate accuracy for questions 1-4 (**number**), assuming that they correspond to texts 1-4. *TODO: Verify!!*
```{r calculate_accuracy}
comprehension <- comprehension_raw %>% select(c("uniform_id", "number", "QUESTIONNUM", "ACCURACY"))

# Only keep numbers 1-4
comprehension <- comprehension[comprehension$number %in% c('1', '2', '3', '4'), ]

# Calculate the comprehension accuracy in %
comprehension_acc <- comprehension %>%
  group_by(uniform_id) %>%
  summarize(
    total_answer = n(),
    correct_answer = sum(ACCURACY),
    accuracy = (correct_answer / total_answer)*100
  )

rm(comprehension_raw, comprehension)
```
Combined files of leap-q, the tests and comprehension data. Comprehension data is preprocessed and only includes accuracy for 1-4.
**Note**: The leap-q file is missing 5 participants: 
- *en_90*
- *ru_2*
- *ru_4*
- *ru_8*
- *ru_9*
This is not a mistake in the code - they are not in the respective excel files.
```{r combine_ids, warning=FALSE}
individual_diffs <- list(comprehension_acc, id_tests, leap_q)      
individual_diffs <- individual_diffs %>% reduce(full_join, by='uniform_id')

individual_diffs <- individual_diffs %>% select(-c(total_answer, correct_answer, subid, lang)) %>% rename(participant_id = uniform_id, comprehension_acc = accuracy) %>% mutate(start_learning = as.integer(start_learning), fluent = as.integer(fluent), reading = as.integer(reading), reading_fluent = as.integer(reading_fluent)) %>% select(participant_id, everything())
```

58 participants have a comprehension score of less than 50% for texts 1-4. **low_accuracy** returns the participant ids.
```{r get_participants_low_acc}
low_accuracy <- comprehension_acc %>% filter(accuracy < 50.0) %>% 
  select(uniform_id) %>%
  distinct()
```

Participants that have an accuarcy below 50% are removed from the participant pool.
```{r participants}
subset_texts <- subset_texts %>%
  anti_join(low_accuracy, by = c("participant_id" = "uniform_id"))

subset_texts %>%
  group_by(language) %>%
  summarise(participant_count = n())

rm(low_accuracy)
```
# Prepare dataframes for export and merge with individual differences
```{r concatenate_texts}
reading_time_data <- bind_rows(text1, text2, text3, text4)
```

```{r remove_participants}
filtered_reading_time_data <- reading_time_data %>%
  filter(participant_id %in% subset_texts$participant_id)

# Remove lang du, tr, gr
filtered_reading_time_data <- filtered_reading_time_data %>%
  filter(!(lang %in% c("du", "gr", "tr")))

filtered_reading_time_data <- filtered_reading_time_data %>% left_join(individual_diffs, by="participant_id")
```

**filtered_reading_time_data** is the preprocessed datafile.
```{r export}
write.csv(filtered_reading_time_data, here("05_output", "reading_times2.csv"), row.names = FALSE)
```

```{r remove_text_dfs}
rm(text1, text2, text3, text4, text5, text6, text7, text8, text9, text10, text11, text12)
```

```{r remove_dfs}
rm(comprehension_acc, leap_q, id_tests, individual_diffs)
rm(eye_table, subset_texts, reading_time_data)
```

