---
title: "Preprocessing"
author: "Natalie-Tieda Hennig"
date: "2023-11-10"
output: 
  html_document:
  toc: true
  toc_float: true
  code_folding: hide
---

# Setup
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

### Define path
- *here* shows the root file - no need to manually set the working directory, which increases reproducibility.
```{r path}
here::i_am("03_Rmd/Preprocessing.Rmd")
library(here)
```

### Libraries
- *dplyr*: table manipulation
- *reticulate*: interoperability between Python and R
```{r libraries, results="hide", message=FALSE}
library(dplyr)
library(reticulate)
use_condaenv("thesis23")
```
**Note** Created a conda environment within the language-and-emotion project file *./thesis23*.
Installing packages using the following command (example *nltk*): *py_install("nltk", "./thesis23")*
To list the downloaded packages use *py_list_packages()*

### Load the data
```{r main_data, results="hide"}
load(here("01_data", "meco L2", "primary data","eye tracking data","joint_data_l2_trimmed.rda"))
```

# Preprocessing
### Create table
1.  Define which columns are relevant. The relevant columns will be used to create a concise table for subsequent analysis.
Note: *Subid* is not needed, because the leap-q files contain a *uniform_id*, which acts as a key. Only **ee.xlsx** does not have a *uniform_id* -> needs further preprocessing.
```{r select_columns}
eye_table <- joint.data %>% select(uniform_id, lang, itemid, ianum, ia, skip, nfix, refix, dur, firstrun.dur, firstrun.gopast, firstrun.gopast.sel, firstfix.dur)
```

2. Rename the columns for better understanding. 
```{r rename_columns}
eye_table <- eye_table %>% rename("participant_id" = "uniform_id", "textnr" = "itemid", "tokenindex" = "ianum", "token"="ia") 
```

#### Sort by token index
Such that the words in the article are ordered. 
```{r ordered_words}
eye_table <- eye_table %>% group_by(textnr, participant_id) %>% arrange(tokenindex, .by_group = TRUE) %>% distinct()
```

#### Ensure data quality
Count the number of words in each text to see if "ia" equals the token.
```{r quality_check_ia, eval=FALSE}
textlength <- eye_table %>% group_by(textnr, participant_id) %>% summarise(length=n(), .groups = "drop")
```

> ```unique(textlength$length)```

```
[1] 161 526 572 126 440 416 432 120 428 392 412 148 530 546 516  98 318 334 107 372 360 350 386 356 142 494 450 474 492 187 670 630 147 540 470 514 173 612 624 610 133 500 444 116 404 382 402
```

There are some duplicates in the dataset. For example, the participant *en_11* contains some rows multiple times which skews the numbers in *textlength*. 
These duplicates must be removed and seem to be an error in the original dataset *joint.data* from the MECO L2.
After adding *distinct()* to the *eye_table* in the chunk **ordered_words**, the duplicates were removed.

> ```x <- tibble(nr = textlength$textnr, tok = textlength$length) %>% distinct() %>% arrange(nr)```

```
# A tibble: 12 Ã— 2
   nr      tok
   <chr> <int>
 1 1       161
 5 2        98
 6 3       107
 7 4       142
 8 5       187
 9 6       147
10 7       173
11 8       133
12 9       116
 2 10      126
 3 11      120
 4 12      148
```
Some texts contain more words than indicated: 

- text 5 should be 185 (=-2)
- text 9 should be 115 (=-1)
- text 12 should be 146 (=-2)

A closer look at the text shows that some words were split up, such as *anti-virus* in text 5, which was counted as two tokens *anti-* and *virus*. 
Since these words have individual measurements, they will be counted as separate words.

#### Split tables according to text number
```{r split_text_by_nr}
text_table <- function(nr){
  return(subset(eye_table, subset = textnr == nr))
}
```

```{r text_by_nr}
text1 <- text_table(1)
text2 <- text_table(2)
text3 <- text_table(3)
text4 <- text_table(4)
text5 <- text_table(5)
text6 <- text_table(6)
text7 <- text_table(7)
text8 <- text_table(8)
text9 <- text_table(9)
text10 <- text_table(10)
text11 <- text_table(11)
text12 <- text_table(12)
```

### Add valency scores to each text
There exist several valency scores.

- Human word ratings
  - ANEW
- Computational (https://doi.org/10.1075/ssol.18002.jac)
  - VADER 
  - SentiArt

First, the columns *textnr*, *tokenindex*, and *token* for each text is extracted. They serve as a key, so that they can be reassigned with the polarity scores to the main table. The output is saved as a csv file.
*distinct()* is used to get the full text only once, instead times the number of participants. It is currently set to *eval=FALSE* because it only needs to run once, to extract the csv files.
```{r export_text_column, eval=FALSE}
text_outputs = 12
for (nr in 1:text_outputs){
  out <- get(paste0("text", nr)) %>% ungroup() %>% select(textnr, tokenindex, token) %>% distinct()
  write.csv(out, here("05_output", paste0(nr, "_text_tokens.csv")))
}
```

#### VADER
```{python import_VADER}
import nltk
from nltk.sentiment.vader import SentimentIntensityAnalyzer
#nltk.download('vader_lexicon')
```

```{python }
import pandas as pd
df = pd.read_csv("/Users/nataliehennig/Documents/language-and-emotion/05_output/1_text_tokens.csv")
df
```


```{python}
# get polarity scores from VADER
sentiments = SentimentIntensityAnalyzer()
df['polarity_score'] = [sentiments.polarity_scores(i)["compound"] for i in df["token"]]

# assign categorical values to the polarity scores
score = df["polarity_score"].values
sentiment = []
pos_threshold = 0.02
neg_threshold = -0.02

for i in score:
    if i >= pos_threshold:
        sentiment.append('Positive')
    elif i <= neg_threshold:
        sentiment.append('Negative')
    else:
        sentiment.append('Neutral')
        
df["sentiment"] = sentiment

#df.drop(df.columns[[4]], axis=1, inplace=True)
df
```
```{python}
df.to_csv("/Users/nataliehennig/Documents/language-and-emotion/05_output/text1_tokens_scores.csv")
```
Problem: A brief glance at the assigned polarity scores shows that the texts are mostly neutral. This is due to the nature of the texts, i.e., the fact that encyclopedic style articles were chosen.

### Merge valency scores to main table
```{r}
read_csv = read.csv(here("05_output", "text1_tokens_scores.csv"))
merger <- read_csv %>% select(tokenindex, token, polarity_score, sentiment)
```

```{r}
total <- merge(text1, merger, by=c("tokenindex", "token")) %>% 
  group_by(textnr, participant_id) %>% 
  arrange(tokenindex, .by_group = TRUE)
```

# Prepare the leap-q xlsx files


# Export data files
TODO: determine table design


