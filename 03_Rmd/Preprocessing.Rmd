---
title: "Preprocessing"
author: "Natalie-Tieda Hennig"
date: "2023-11-10"
output: 
  html_document:
  toc: true
  toc_float: true
  code_folding: hide
---

# Setup
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

### Define path
- *here* shows the root file - no need to manually set the working directory, which increases reproducibility.
```{r path}
here::i_am("03_Rmd/Preprocessing.Rmd")
library(here)
```

### Libraries
- *dplyr*: table manipulation
- *reticulate*: interoperability between Python and R
- *quanteda* and *quanteda.sentiment*: statistical text operations, ANEW ratings
- *tidyr* table manipulation
```{r libraries, results="hide", message=FALSE}
library(dplyr)
library(reticulate)
library(quanteda)
library(quanteda.sentiment)
library(tidyr)
library(spacyr)
use_condaenv("thesis23")
```
**Note** Created a conda environment within the language-and-emotion project file *./thesis23*.
Installing packages using the following command (example *nltk*): *py_install("nltk", "./thesis23")*
To list the downloaded packages use *py_list_packages()*

### Load the data
```{r main_data, results="hide"}
load(here("01_data", "meco L2", "primary data","eye tracking data","joint_data_l2_trimmed.rda"))
```

# Preprocessing
### Create table
1.  Define which columns are relevant. The relevant columns will be used to create a concise table for subsequent analysis.
Note: *Subid* is not needed, because the leap-q files contain a *uniform_id*, which acts as a key. Only **ee.xlsx** does not have a *uniform_id* -> needs further preprocessing.
```{r select_columns}
eye_table <- joint.data %>% select(uniform_id, lang, itemid, ianum, ia, skip, nfix, refix, dur, firstrun.dur, firstrun.gopast, firstrun.gopast.sel, firstfix.dur)
```

2. Rename the columns for better understanding. 
```{r rename_columns}
eye_table <- eye_table %>% rename("participant_id" = "uniform_id", "textnr" = "itemid", "tokenindex" = "ianum", "token"="ia") 
```

#### Sort by token index
Such that the words in the article are ordered. 
```{r ordered_words}
eye_table <- eye_table %>% group_by(textnr, participant_id) %>% arrange(tokenindex, .by_group = TRUE) %>% distinct()
```


#### Add POS-Tags
spacy_parse *(spaCy Version: 3.4.1, language model: en_core_web_sm)* is used to add POS tags to the words. It contains both a detailed set *tags*, as well as a less fine-grained universal set *pos*.
The POS tags are based on the **Penn Treebank** tag set.
```{r parsing, echo=TRUE, message=FALSE, results='hide'}
spacy_install(envname = "/Users/nataliehennig/Documents/language-and-emotion/thesis23", python_version = "3.10")
spacy_initialize(condaenv = "./thesis23")

POS = spacy_parse(eye_table$token, tag=TRUE)
TAG = POS$tag 
```

#### Ensure data quality
Count the number of words in each text to see if "ia" equals the token.
```{r quality_check_ia, eval=FALSE}
textlength <- eye_table %>% group_by(textnr, participant_id) %>% summarise(length=n(), .groups = "drop")
```

> ```unique(textlength$length)```

```
[1] 161 526 572 126 440 416 432 120 428 392 412 148 530 546 516  98 318 334 107 372 360 350 386 356 142 494 450 474 492 187 670 630 147 540 470 514 173 612 624 610 133 500 444 116 404 382 402
```

There are some duplicates in the dataset. For example, the participant *en_11* contains some rows multiple times which skews the numbers in *textlength*. 
These duplicates must be removed and seem to be an error in the original dataset *joint.data* from the MECO L2.
After adding *distinct()* to the *eye_table* in the chunk **ordered_words**, the duplicates were removed.

> ```x <- tibble(nr = textlength$textnr, tok = textlength$length) %>% distinct() %>% arrange(nr)```

```
# A tibble: 12 × 2
   nr      tok
   <chr> <int>
 1 1       161
 5 2        98
 6 3       107
 7 4       142
 8 5       187
 9 6       147
10 7       173
11 8       133
12 9       116
 2 10      126
 3 11      120
 4 12      148
```
Some texts contain more words than indicated: 

- text 5 should be 185 (=-2)
- text 9 should be 115 (=-1)
- text 12 should be 146 (=-2)

A closer look at the text shows that some words were split up, such as *anti-virus* in text 5, which was counted as two tokens *anti-* and *virus*. 
Since these words have individual measurements, they will be counted as separate words.

#### Split tables according to text number
```{r split_text_by_nr}
text_table <- function(nr){
  return(subset(eye_table, subset = textnr == nr))
}
```

```{r text_by_nr}
text1 <- text_table(1)
text2 <- text_table(2)
text3 <- text_table(3)
text4 <- text_table(4)
text5 <- text_table(5)
text6 <- text_table(6)
text7 <- text_table(7)
text8 <- text_table(8)
text9 <- text_table(9)
text10 <- text_table(10)
text11 <- text_table(11)
text12 <- text_table(12)
```

### Add valency scores to each text
There exist several valency scores. Hutto & Gilbert (2014) provide a rundown of currently used polarity and semantic intensity lexicons.

- Human word ratings
  - ANEW
- Computational (https://doi.org/10.1075/ssol.18002.jac)
  - VADER 
  - SentiArt

First, the columns *textnr*, *tokenindex*, and *token* for each text is extracted. They serve as a key, so that they can be reassigned with the polarity scores to the main table. The output is saved as a csv file.
*distinct()* is used to get the full text only once, instead times the number of participants. It is currently set to *eval=FALSE* because it only needs to run once, to extract the csv files.
```{r export_text_column, eval=FALSE}
text_outputs = 12
for (nr in 1:text_outputs){
  out <- get(paste0("text", nr)) %>% ungroup() %>% select(textnr, tokenindex, token) %>% distinct()
  write.csv(out, here("05_output", paste0(nr, "_text_tokens.csv")))
}
```

#### VADER
```{r call_vader, eval=FALSE}
source_python(here("04_scripts", "01_vader.py"))

for (nr in 1:text_outputs){
  assign_polarity_score(nr)
}
```
*assign_polarity_scores* has positive and negative values as optional argument. The default is set to *0.02*/*-0.02*. Files are saved in 05_output as *nr_vader_scores.csv*.
Problem: A brief glance at the assigned polarity scores shows that the texts are mostly neutral. This is due to the nature of the texts, i.e., the fact that encyclopedic style articles were chosen.

Attach the valency scores to the main tables.
```{r merge_vader}

for (i in 1:12) { 
  file_path <- here("05_output", paste0(i, "_vader_scores.csv"))
  
  # Check if the file exists before attempting to read it
  if (file.exists(file_path)) {
    vader_scores <- read.csv(file_path)
    merger <- vader_scores %>% select(tokenindex, token, vader_score, vader_polarity)
    
    text_name <- paste0("text", i)
    
    # Merge the data frame with its corresponding VADER scores
    text_df <- get(text_name)
    text_df <- merge(text_df, merger, by = c("tokenindex", "token")) %>%
      group_by(textnr, participant_id) %>%
      arrange(tokenindex, .by_group = TRUE)
    
    # Update the original data frame
    assign(text_name, text_df, envir = .GlobalEnv)
  } else {
    cat("File", file_path, "does not exist.\n")
  }
}
```

#### ANEW
Valence ratings in which humans rated words with values for pleasure, dominance and arousal. 
- Based on Bradley and Lang (1999): Relatively small dataset of 1034 words. 
- Bradley and Lang (2009?): 2477 words.
- The quanteda.sentiment ANEW dataset contains 2741 words.
There exist several updated/extended versions, such as Warriner et al. (2013) word rating lexicon using Amazon Mechanical Turk, as well as an expansion technique, named ANEW+, as proposed by Shaikh et al. (2016).

```{r call_anew}
source(here("04_scripts","02_anew.R"), local = knitr::knit_global())
#lapply(valence(data_dictionary_ANEW), head)
#pleasure <- valence(data_dictionary_ANEW)[["pleasure"]]

texts_listed <- list("text1", "text2", "text3", "text4", "text5", "text6", "text7", "text8", "text9", "text10", "text11", "text12")

for (text_name in texts_listed) {
  add_anew_score(text_name)
}

```
Words that are not in the dictionary are automatically assigned a value of 0.00 which needs to be taken into consideration. Therefore, POS-Tags need to be added to compute the coverage for only the content words.

```{r calculate coverage}
print("Coverage")

for (text_name in texts_listed) {
  text_in <- get(text_name)
  coverage_anew = sum(text_in$anew_score != 0.00) / nrow(text_in) *100
  coverage_vader = sum(text_in$vader_score != 0.00) / nrow(text_in) *100
  print(paste0(text_name, " - ", "ANEW: ", round(coverage_anew,0), "%, ", "VADER: ", round(coverage_vader,0), "%"))
}
```

#### SentiArt
"Words with high and low AAP values (e.g. “nature” versus “criminal”) are more likely to induce positive and negative emotions, respectively, and words with AAP values approaching zero are considered to be more neutral (e.g. “bottle”). AAP has been validated as an accurate predictor of self-reported valence ratings of single words, and has been found to predict emotional states during story reading better than valence ratings from an affective word database" (Jacobs & Kinder, 2019)" (Lei, Willems & Eekhof 2023: 999).

- Uses Vector Space Models instead of emotional dictionaries
- High predictive accuracy
- Pre-trained on the wiki.en corpus, particularly suitable for the texts under investigation
- expandable to different languages

"it was shown that the computational AAP values predicted ~2800 human database (subtlex). In a first cross-validation study, it was shown that the computational AAP values
valence ratings from the BAWL better than the computational valence values, establishing the SDEWAC predicted ~2800 human valence ratings from the BAWL better than the computational valence values,VSM as the best-fitting of the three VSMs (R2 > 0.5, r = 0.72, p < 0.00012)." (Jacobs & Kinder 2019) 

# Additional Metrics
Lexical Features

- Word length
- Word frequency
- POS
- Punctuation (binary)
- Distance to last word
- Levenshtein Distance?
- Predictability in context?

Between-Group Features

- Language Distance 

Individual Features

- AoA

# Prepare the leap-q xlsx files


# Export data files



