---
title: "Preprocessing"
author: "Natalie-Tieda Hennig"
date: "2023-11-10"
output: 
  html_document:
  toc: true
  toc_float: true
  code_folding: hide
---

# Setup

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Define path

```{r path}
here::i_am("03_Rmd/Preprocessing.Rmd")
library(here)
```

## Libraries

```{r libraries}
library(dplyr)
```

## Load the data

```{r main_data}
load(here("01_data", "meco L2", "primary data","eye tracking data","joint_data_l2_trimmed.rda"))
```

# Preprocessing

## Create table

1.  Define which columns are relevant. The relevant columns will be used to create a concise table for subsequent analysis.
Note: Subid is not needed, because the leap-q files contain a uniform_id, which acts as a key. Only "ee.xlsx" does not have a uniform_id -> needs further preprocessing.
```{r}
eye_table <- joint.data %>% select(uniform_id, lang, itemid, ianum, ia, skip, nfix, refix, dur, firstrun.dur, firstrun.gopast, firstrun.gopast.sel, firstfix.dur)
```

2. Rename the columns for better understanding. 
```{r}
eye_table <- eye_table %>% rename("participant_id" = "uniform_id", "textnr" = "itemid", "tokenindex" = "ianum", "token"="ia") 
```

## Sort by token index
Such that the words in the article are ordered.
```{r}
eye_table <- eye_table %>% group_by(textnr, participant_id) %>% arrange(tokenindex, .by_group = TRUE) %>% distinct()
```

Count the number of words in each text to see if "ia" equals the token.
```{r}
textlength <- eye_table %>% group_by(textnr, participant_id) %>% summarise(length=n(), .groups = "drop")
```

> unique(textlength$length)
 [1] 161 526 572 126 440 416 432 120 428 392 412 148 530 546 516  98 318 334 107 372 360 350 386 356 142 494 450 474 492 187 670 630 147 540 470 514 173 612 624 610 133 500 444 116
[45] 404 382 402

There are some duplicates in the dataset. For example, the participant "en_11" contains some rows multiple times which skews the textlength. These duplicates must be removed and seem to be an error in the original dataset "joint.data" from the MECO L2.

After adding distinct() to the eye_table, the duplicates were removed.

> x <- tibble(nr = textlength$textnr, tok = textlength$length) %>% distinct() %>% arrange(nr)

# A tibble: 12 Ã— 2
   nr      tok
   <chr> <int>
 1 1       161
 5 2        98
 6 3       107
 7 4       142
 8 5       187
 9 6       147
10 7       173
11 8       133
12 9       116
 2 10      126
 3 11      120
 4 12      148
 
Some texts contain more words than indicated: 
- text 5 should be 185 (=-2)
- text 9 should be 115 (=-1)
- text 12 should be 146 (=-2)

A closer look at the text shows that some words were split up, such as anti-virus in text 5, which was counted as two tokens "anti-" and "virus". Since these words have individual measurements, they will be counted as separate words.

## Split tables according to text number
```{r}
text1 <- subset(eye_table, subset = textnr == "1")
text2 <- subset(eye_table, subset = textnr == "2")
text3 <- subset(eye_table, subset = textnr == "3")
text4 <- subset(eye_table, subset = textnr == "4")
text5 <- subset(eye_table, subset = textnr == "5")
text6 <- subset(eye_table, subset = textnr == "6")
text7 <- subset(eye_table, subset = textnr == "7")
text8 <- subset(eye_table, subset = textnr == "8")
text9 <- subset(eye_table, subset = textnr == "9")
text10 <- subset(eye_table, subset = textnr == "10")
text11 <- subset(eye_table, subset = textnr == "11")
text12 <- subset(eye_table, subset = textnr == "12")
```


## Add valency scores to each text
TODO: determine which valency score to use / compare different ones?
- ANEW
- Computational method: e.g. VADER (https://www.jbe-platform.com/content/journals/10.1075/ssol.18002.jac)


## Export data files
TODO: determine table design


## Merge the leap-q xlsx files

